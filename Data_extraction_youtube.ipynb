{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be266f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import SRTFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the YouTube API client\n",
    "api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55192587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids_with_captions(query, max_results=50):\n",
    "    video_ids = []\n",
    "    # search request to the YouTube API\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        part='id',\n",
    "        maxResults=max_results,\n",
    "        type='video',\n",
    "        videoCaption='closedCaption',  # Only retrieve videos with captions\n",
    "    ).execute()\n",
    "\n",
    "    # Extract video IDs\n",
    "    for search_result in search_response.get('items', []):\n",
    "        video_ids.append(search_result['id']['videoId'])\n",
    "\n",
    "    return video_ids\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitize the filename by removing or replacing invalid characters.\"\"\"\n",
    "    valid_filename = re.sub(r'[^\\w\\s-]', '', filename).strip().lower()\n",
    "    return re.sub(r'[-\\s]+', '-', valid_filename)\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YouTubeDataAnalysis\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    query = \"hindi old comedy movies\"  \n",
    "    video_ids = get_video_ids_with_captions(query)\n",
    "\n",
    "    # Converting the video IDs into a Spark DataFrame\n",
    "    rows = [Row(video_id=video_id) for video_id in video_ids]\n",
    "    df = spark.createDataFrame(rows)\n",
    "\n",
    "    #counting the number of video IDs\n",
    "    print(f\"Total video IDs: {df.count()}\")\n",
    "\n",
    "    # Save the DataFrame to HDFS\n",
    "    hdfs_path = f\"hdfs://namenode:9000/video_ids{sanitize_filename(query)}.csv\"\n",
    "    df.write.csv(hdfs_path)\n",
    "\n",
    "    print(f\"Saved {len(video_ids)} video IDs to {hdfs_path}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74776a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_transcript(video_id):\n",
    "    \"\"\"\n",
    "    Fetches the transcript for a given video ID and returns it as SRT format.\n",
    "    This function is intended to be used as a UDF in Spark.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Fetching the transcript in English\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
    "        \n",
    "        # Formatting the transcript as SRT\n",
    "        formatter = SRTFormatter()\n",
    "        srt_transcript = formatter.format_transcript(transcript)\n",
    "        \n",
    "        return srt_transcript\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"YouTubeTranscripts\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Define the UDF for downloading transcripts\n",
    "    download_transcript_udf = udf(download_transcript, StringType())\n",
    "\n",
    "    # Reading a list of video IDs from HDFS \n",
    "    hdfs_input_path = \"hdfs://namenode:9000/video_ids\"\n",
    "    df_video_ids = spark.read.text(hdfs_input_path)\n",
    "\n",
    "    # Applying the UDF to download transcripts\n",
    "    df_transcripts = df_video_ids.withColumn(\"transcript\", download_transcript_udf(\"value\"))\n",
    "\n",
    "\n",
    "    # Saving transcripts to HDFS\n",
    "    hdfs_output_path = \"hdfs://namenode:9000/subtitles/\"\n",
    "    df_transcripts.write.text(hdfs_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be498c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
