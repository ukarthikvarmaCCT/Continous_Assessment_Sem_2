{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313bc9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All subtitles have been processed and saved to a single CSV file.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove the specified word \"[संगीत]\" from the text\n",
    "    text = re.sub(r'\\[संगीत\\]', '', text)\n",
    "    # Remove all non-alphabetic characters (except spaces) and strip extra spaces\n",
    "    return re.sub(r'[^ऀ-ॿ ]', '', text).strip()\n",
    "\n",
    "def process_subtitle_file(srt_file_path):\n",
    "    subtitles = []\n",
    "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "    for i in range(0, len(content), 4):\n",
    "        if i + 2 < len(content):  # Ensure there's a subtitle text line\n",
    "            cleaned_line = clean_text(content[i + 2])\n",
    "            if cleaned_line:  # Only add non-empty lines\n",
    "                subtitles.append(cleaned_line)\n",
    "    return subtitles\n",
    "\n",
    "# Path to the folder containing the SRT files\n",
    "subtitles_folder_path = 'subtitles'\n",
    "\n",
    "# Initialize an empty list to store subtitles from all SRT files\n",
    "all_subtitles = []\n",
    "\n",
    "# Iterate through each SRT file in the subtitles folder\n",
    "for filename in os.listdir(subtitles_folder_path):\n",
    "    if filename.endswith(\".srt\"):\n",
    "        srt_file_path = os.path.join(subtitles_folder_path, filename)\n",
    "        subtitles = process_subtitle_file(srt_file_path)\n",
    "        # Append subtitles from the current file to the all_subtitles list\n",
    "        all_subtitles.extend(subtitles)\n",
    "\n",
    "# Creating a DataFrame from all the cleaned subtitles\n",
    "df_subtitles = pd.DataFrame(all_subtitles, columns=['Hindi'])\n",
    "\n",
    "# Path where the combined CSV file will be saved\n",
    "combined_csv_file_path = 'combined_subtitles.csv'\n",
    "\n",
    "# Save the DataFrame to a single CSV file\n",
    "df_subtitles.to_csv(combined_csv_file_path, index=False)\n",
    "\n",
    "print(\"All subtitles have been processed and saved to a single CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample of the file to inspect its content and structure\n",
    "file_path = '/mnt/data/combined_subtitles.csv'\n",
    "df_sample = pd.read_csv(file_path, nrows=100)  # Load a small sample for inspection\n",
    "\n",
    "df_sample.info(), df_sample.head()\n",
    "\n",
    "# Remove duplicates and perform basic text normalization on the 'Hindi' column.\n",
    "# Text normalization includes trimming leading and trailing whitespaces and replacing multiple spaces with a single space.\n",
    "\n",
    "# Remove duplicates\n",
    "df_no_duplicates = df_sample.drop_duplicates()\n",
    "\n",
    "# Normalize text: trim and reduce multiple spaces to one\n",
    "df_no_duplicates['Hindi'] = df_no_duplicates['Hindi'].str.strip().replace('\\s+', ' ', regex=True)\n",
    "\n",
    "df_no_duplicates.info(), df_no_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8f2378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/apple/anaconda3/lib/python3.11/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the translation model pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "\n",
    "# Path to your large CSV file\n",
    "csv_file_path = 'combined_subtitles.csv'\n",
    "\n",
    "def translate_text(hindi_text):\n",
    "    # Translate text and return the translation\n",
    "    translation = translator(hindi_text, max_length=512)\n",
    "    return translation[0]['translation_text']\n",
    "\n",
    "def get_total_rows(file_path):\n",
    "    # Efficiently count the total number of rows in the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        total_rows = sum(1 for _ in file) - 1  # Subtract 1 for the header\n",
    "    return total_rows\n",
    "\n",
    "def process_file(file_path, chunksize):\n",
    "    # Process the CSV in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        chunk['English'] = chunk['Hindi'].apply(translate_text)\n",
    "        # Append to a new file or overwrite. Adjust the mode and file name as needed.\n",
    "        chunk.to_csv(file_path.replace('.csv', '_translated.csv'), mode='a', index=False, header=not Path(file_path.replace('.csv', '_translated.csv')).exists())\n",
    "\n",
    "# Calculate the chunk size as 1/3rd of the total number of rows\n",
    "total_rows = get_total_rows(csv_file_path)\n",
    "chunk_size = max(1, total_rows // 3)  # Ensure at least 1 to avoid division by zero\n",
    "\n",
    "process_file(csv_file_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be366640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_hinglish(hindi, english):\n",
    "    english_words = english.split()\n",
    "    hindi_words = hindi.split()\n",
    "    # Decide the number of Hindi words to insert (up to 3 words or the length of the Hindi sentence)\n",
    "    num_inserts = min(3, len(hindi_words))\n",
    "    for _ in range(num_inserts):\n",
    "        if english_words:  # Ensure there are English words left to insert into\n",
    "            insert_pos = random.randint(0, len(english_words))\n",
    "            hindi_word_to_insert = random.choice(hindi_words)\n",
    "            english_words.insert(insert_pos, hindi_word_to_insert)\n",
    "            # Optional: remove the Hindi word from future consideration (if you want each Hindi word used only once)\n",
    "            hindi_words.remove(hindi_word_to_insert)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "# Apply the function to create a Hinglish column\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish(row['Hindi'], row['English']), axis=1)\n",
    "\n",
    "# Displaying the DataFrame with the new Hinglish sentences\n",
    "df_subtitles[['Hindi', 'English', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec9235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_hinglish_based_on_stopwords(english, hindi):\n",
    "    words = english.split()\n",
    "    stop_word_indices = [index for index, word in enumerate(words) if word.lower() in stop_words]\n",
    "\n",
    "    if not stop_word_indices:\n",
    "        # No stop words found, return the original sentence (you can decide how to handle this case)\n",
    "        return english\n",
    "\n",
    "    # Find pivot index for Hindi insertion\n",
    "    pivot_index = stop_word_indices[-1] + 1  # We split the sentence after the last stop word\n",
    "\n",
    "    english_part = \" \".join(words[:pivot_index])\n",
    "    hindi_part = \" \".join(hindi.split()[pivot_index:])  # Corresponding Hindi split\n",
    "\n",
    "    # Combine the English and Hindi parts\n",
    "    hinglish_sentence = english_part + ' ' + hindi_part\n",
    "    return hinglish_sentence\n",
    "\n",
    "# Apply the function to create the Hinglish sentences\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish_based_on_stopwords(row['English'], row['Hindi']), axis=1)\n",
    "\n",
    "df_subtitles[['English', 'Hindi', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a7d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def split_on_last_stopword(sentence, stop_words):\n",
    "    words = sentence.split()\n",
    "    last_stop_word_index = None\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in stop_words:\n",
    "            last_stop_word_index = i\n",
    "\n",
    "    if last_stop_word_index is not None and last_stop_word_index + 1 < len(words):\n",
    "        return \" \".join(words[:last_stop_word_index + 1]), \" \".join(words[last_stop_word_index + 1:])\n",
    "    else:\n",
    "        return sentence, \"\"\n",
    "\n",
    "# Splitting the English sentences and creating a new column for the part to be translated\n",
    "df_subtitles['English_Part'] = \"\"\n",
    "df_subtitles['To_Translate'] = \"\"\n",
    "\n",
    "for index, row in df_subtitles.iterrows():\n",
    "    english_part, to_translate = split_on_last_stopword(row['English'], stop_words)\n",
    "    df_subtitles.at[index, 'English_Part'] = english_part\n",
    "    df_subtitles.at[index, 'To_Translate'] = to_translate\n",
    "\n",
    "# Initialize the translation model pipeline\n",
    "translator = pipeline(\"translation_en_to_hi\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "def translate_to_hindi(text):\n",
    "    if text.strip():  # Check if the text is not just empty or whitespace\n",
    "        translation = translator(text, max_length=512)[0]['translation_text']\n",
    "        return translation\n",
    "    return \"\"\n",
    "# Simulating the translation (here we just reverse the order of words to mimic translation)\n",
    "df_subtitles['Translated'] = df_subtitles['To_Translate'].apply(translate_to_hindi)\n",
    "\n",
    "# Combining the English part and the \"translated\" Hindi to form Hinglish sentences\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: row['English_Part'] + ' ' + row['Translated'], axis=1)\n",
    "\n",
    "df_subtitles[['English', 'Hindi', 'English_Part', 'To_Translate', 'Translated', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b18ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
