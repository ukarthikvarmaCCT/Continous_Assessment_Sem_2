{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080c333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/apple/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers in /Users/apple/anaconda3/lib/python3.11/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932d3c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 13:21:30.769016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import concurrent.futures\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a8381",
   "metadata": {},
   "source": [
    "## Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313bc9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. CSV files have been combined based on video IDs and duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[संगीत\\]', '', text)\n",
    "    return re.sub(r'[^ऀ-ॿ ]', '', text).strip()\n",
    "\n",
    "def process_subtitle_file(srt_file_path):\n",
    "    subtitles = []\n",
    "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "    for i in range(0, len(content), 4):\n",
    "        if i + 2 < len(content):\n",
    "            cleaned_line = clean_text(content[i + 2])\n",
    "            if cleaned_line and len(cleaned_line.split()) > 1:\n",
    "                subtitles.append([cleaned_line])\n",
    "    return subtitles\n",
    "\n",
    "def save_subtitles_to_csv(video_id, subtitles, subtitles_csv_dir):\n",
    "    if subtitles:\n",
    "        df_subtitles = pd.DataFrame(subtitles, columns=['Hindi'])\n",
    "        csv_file_path = os.path.join(subtitles_csv_dir, f\"{video_id}.csv\")\n",
    "        df_subtitles.to_csv(csv_file_path, index=False)\n",
    "\n",
    "def process_file(filename, subtitles_dir, subtitles_csv_dir):\n",
    "    if filename.endswith(\".srt\"):\n",
    "        video_id = filename[:-4]\n",
    "        srt_file_path = os.path.join(subtitles_dir, filename)\n",
    "        subtitles = process_subtitle_file(srt_file_path)\n",
    "        save_subtitles_to_csv(video_id, subtitles, subtitles_csv_dir)\n",
    "\n",
    "def read_video_ids(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def combine_csv_files(video_ids, output_file, subtitles_csv_dir):\n",
    "    csv_frames = []\n",
    "    for video_id in video_ids:\n",
    "        csv_path = os.path.join(subtitles_csv_dir, f\"{video_id}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            csv_frames.append(df)\n",
    "    combined_df = pd.concat(csv_frames)\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "def parallel_process_files(files, subtitles_dir, subtitles_csv_dir):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_file, filename, subtitles_dir, subtitles_csv_dir) for filename in files]\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "# Example usage\n",
    "directories = {\n",
    "    \"video_ids_dir\": 'video_ids',\n",
    "    \"subtitles_dir\": 'subtitles',\n",
    "    \"subtitles_csv_dir\": 'Subtitles_csv',\n",
    "    \"output_dir\": 'Subtitles_csv_combined'\n",
    "}\n",
    "\n",
    "for dir_path in directories.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "files = os.listdir(directories['subtitles_dir'])\n",
    "parallel_process_files(files, directories['subtitles_dir'], directories['subtitles_csv_dir'])\n",
    "\n",
    "for filename in os.listdir(directories['video_ids_dir']):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        base_name = filename[:-4]\n",
    "        output_filename = f\"{base_name}_combined_data.csv\"\n",
    "        output_path = os.path.join(directories['output_dir'], output_filename)\n",
    "        video_ids_path = os.path.join(directories['video_ids_dir'], filename)\n",
    "        video_ids = read_video_ids(video_ids_path)\n",
    "        combine_csv_files(video_ids, output_path, directories['subtitles_csv_dir'])\n",
    "\n",
    "print(\"Process completed. CSV files have been combined based on video IDs and duplicates removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40da7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to count words in a sentence\n",
    "def count_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "# Base directory containing your CSV files\n",
    "directory_path = 'Subtitles_csv_combined'\n",
    "\n",
    "# Placeholder for combined data\n",
    "combined_data = pd.DataFrame(columns=['Hindi'])\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter sentences with 5 or more words\n",
    "        filtered_df = df[df['Hindi'].apply(count_words) >= 5]\n",
    "        \n",
    "        # Combine\n",
    "        combined_data = pd.concat([combined_data, filtered_df])\n",
    "\n",
    "# If the combined data has more than 2000 sentences, randomly select 2000\n",
    "if len(combined_data) > 2000:\n",
    "    combined_data = combined_data.sample(n=2000, random_state=1).reset_index(drop=True)\n",
    "else:\n",
    "    combined_data = combined_data.sample(frac=1, random_state=1).reset_index(drop=True)  # Shuffle if less than 2000\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_file_path = 'combined_for_translation.csv'\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f'Combined file saved to {combined_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf9ee6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>वाली डोंगल एंड तीन डिवाइसेज से कनेक्ट हो</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>धन्यवाद इस वीडियो को स्पॉन्सर करने के</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>थे एक समय की बात है की इब्राहिम के पिता</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>बेसिक्स वाज टू ऑलवेज इवॉल्व लाइक ही सेड</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>आप यहां सोचोगे कि सोरा यह सब कर कैसे पा</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Hindi\n",
       "0  वाली डोंगल एंड तीन डिवाइसेज से कनेक्ट हो\n",
       "1     धन्यवाद इस वीडियो को स्पॉन्सर करने के\n",
       "2   थे एक समय की बात है की इब्राहिम के पिता\n",
       "3   बेसिक्स वाज टू ऑलवेज इवॉल्व लाइक ही सेड\n",
       "4   आप यहां सोचोगे कि सोरा यह सब कर कैसे पा"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"combined_for_translation.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ce7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved back to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the translation pipeline\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-hi-en')\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translation = translator(text, max_length=400)[0]['translation_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        translation = \"\"\n",
    "    return translation\n",
    "\n",
    "# Parallel translation function\n",
    "def parallel_translate(df, num_workers=5):\n",
    "    texts = df['Hindi'].tolist()\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_text = {executor.submit(translate_text, text): text for text in texts}\n",
    "        for future in as_completed(future_to_text):\n",
    "            text = future_to_text[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'{text} generated an exception: {exc}')\n",
    "            else:\n",
    "                df.loc[df['Hindi'] == text, 'English'] = result\n",
    "    return df\n",
    "\n",
    "# Path to your combined CSV file\n",
    "file_path = 'combined_for_translation.csv'\n",
    "\n",
    "# Read the combined CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'English' column exists\n",
    "df['English'] = ''\n",
    "\n",
    "# Translate in parallel and update the DataFrame\n",
    "df_updated = parallel_translate(df)\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df_updated.to_csv(file_path, index=False)\n",
    "print(f'Translation completed and saved back to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423df682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            English  \\\n",
      "0     Connect to the misleadr and three devicesages   \n",
      "1                   Thanks for cursoring this video   \n",
      "2                      For a time, Abraham's father   \n",
      "3                 Basics Wadge Toals Torrite Expend   \n",
      "4         You'll think here how Soira can do it all   \n",
      "...                                             ...   \n",
      "5995                                            NaN   \n",
      "5996                                            NaN   \n",
      "5997                                            NaN   \n",
      "5998                                            NaN   \n",
      "5999                                            NaN   \n",
      "\n",
      "                       preprocessed_text         text  \n",
      "0     connect misleadr three devicesages          NaN  \n",
      "1                 thanks cursoring video          NaN  \n",
      "2                    time abraham father          NaN  \n",
      "3       basic wadge toals torrite expend          NaN  \n",
      "4                      youll think soira          NaN  \n",
      "...                                  ...          ...  \n",
      "5995                                         a  a a    \n",
      "5996                            aa thats  aa  thats    \n",
      "5997                             z thats    z thats a  \n",
      "5998                                             a  a  \n",
      "5999                                              a a  \n",
      "\n",
      "[6000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# NLTK downloads\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    stop_words = set(stopwords.words('english'))  # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()  # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Augmentation Function - Synonym Replacement\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' a-zA-Z'])\n",
    "            synonyms.add(synonym)  \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def augment_sentence(sentence, n_augment=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    synonyms = {word: get_synonyms(word) for word in tokens}\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    for _ in range(n_augment):\n",
    "        augmented_tokens = tokens.copy()\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word in synonyms and len(synonyms[word]) > 0:\n",
    "                synonym = random.choice(synonyms[word])\n",
    "                augmented_tokens[i] = synonym\n",
    "        augmented_sentences.append(' '.join(augmented_tokens))\n",
    "    return augmented_sentences\n",
    "\n",
    "# Load your dataset\n",
    "# For demonstration, replace this with loading your actual dataset\n",
    "data = pd.read_csv(\"combined_for_translation.csv\")\n",
    "\n",
    "# Apply Preprocessing\n",
    "data['preprocessed_text'] = data['English'].apply(preprocess_text)\n",
    "\n",
    "# Apply Augmentation\n",
    "n_augment = 2  # Number of augmented versions to generate\n",
    "augmented_texts = []\n",
    "for sentence in data['preprocessed_text']:\n",
    "    augmented_texts.extend(augment_sentence(sentence, n_augment=n_augment))\n",
    "\n",
    "# Create a DataFrame for Augmented Texts\n",
    "augmented_data = pd.DataFrame({'text': augmented_texts})\n",
    "augmented_data['preprocessed_text'] = augmented_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Combine original and augmented datasets\n",
    "combined_data = pd.concat([data[['English', 'preprocessed_text']], augmented_data]).reset_index(drop=True)\n",
    "\n",
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d9b041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "      <th>English</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>वाली डोंगल एंड तीन डिवाइसेज से कनेक्ट हो</td>\n",
       "      <td>Connect to the misleadr and three devicesages</td>\n",
       "      <td>connect misleadr three devicesages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>धन्यवाद इस वीडियो को स्पॉन्सर करने के</td>\n",
       "      <td>Thanks for cursoring this video</td>\n",
       "      <td>thanks cursoring video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>थे एक समय की बात है की इब्राहिम के पिता</td>\n",
       "      <td>For a time, Abraham's father</td>\n",
       "      <td>time abraham father</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>बेसिक्स वाज टू ऑलवेज इवॉल्व लाइक ही सेड</td>\n",
       "      <td>Basics Wadge Toals Torrite Expend</td>\n",
       "      <td>basic wadge toals torrite expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>आप यहां सोचोगे कि सोरा यह सब कर कैसे पा</td>\n",
       "      <td>You'll think here how Soira can do it all</td>\n",
       "      <td>youll think soira</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Hindi  \\\n",
       "0  वाली डोंगल एंड तीन डिवाइसेज से कनेक्ट हो   \n",
       "1     धन्यवाद इस वीडियो को स्पॉन्सर करने के   \n",
       "2   थे एक समय की बात है की इब्राहिम के पिता   \n",
       "3   बेसिक्स वाज टू ऑलवेज इवॉल्व लाइक ही सेड   \n",
       "4   आप यहां सोचोगे कि सोरा यह सब कर कैसे पा   \n",
       "\n",
       "                                         English  \\\n",
       "0  Connect to the misleadr and three devicesages   \n",
       "1                Thanks for cursoring this video   \n",
       "2                   For a time, Abraham's father   \n",
       "3              Basics Wadge Toals Torrite Expend   \n",
       "4      You'll think here how Soira can do it all   \n",
       "\n",
       "                    preprocessed_text  \n",
       "0  connect misleadr three devicesages  \n",
       "1              thanks cursoring video  \n",
       "2                 time abraham father  \n",
       "3    basic wadge toals torrite expend  \n",
       "4                   youll think soira  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be366640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_hinglish(hindi, english):\n",
    "    english_words = english.split()\n",
    "    hindi_words = hindi.split()\n",
    "    # Decide the number of Hindi words to insert (up to 3 words or the length of the Hindi sentence)\n",
    "    num_inserts = min(3, len(hindi_words))\n",
    "    for _ in range(num_inserts):\n",
    "        if english_words:  # Ensure there are English words left to insert into\n",
    "            insert_pos = random.randint(0, len(english_words))\n",
    "            hindi_word_to_insert = random.choice(hindi_words)\n",
    "            english_words.insert(insert_pos, hindi_word_to_insert)\n",
    "            # Optional: remove the Hindi word from future consideration (if you want each Hindi word used only once)\n",
    "            hindi_words.remove(hindi_word_to_insert)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "# Apply the function to create a Hinglish column\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish(row['Hindi'], row['English']), axis=1)\n",
    "\n",
    "# Displaying the DataFrame with the new Hinglish sentences\n",
    "df_subtitles[['Hindi', 'English', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec9235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_hinglish_based_on_stopwords(english, hindi):\n",
    "    words = english.split()\n",
    "    stop_word_indices = [index for index, word in enumerate(words) if word.lower() in stop_words]\n",
    "\n",
    "    if not stop_word_indices:\n",
    "        # No stop words found, return the original sentence (you can decide how to handle this case)\n",
    "        return english\n",
    "\n",
    "    # Find pivot index for Hindi insertion\n",
    "    pivot_index = stop_word_indices[-1] + 1  # We split the sentence after the last stop word\n",
    "\n",
    "    english_part = \" \".join(words[:pivot_index])\n",
    "    hindi_part = \" \".join(hindi.split()[pivot_index:])  # Corresponding Hindi split\n",
    "\n",
    "    # Combine the English and Hindi parts\n",
    "    hinglish_sentence = english_part + ' ' + hindi_part\n",
    "    return hinglish_sentence\n",
    "\n",
    "# Apply the function to create the Hinglish sentences\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish_based_on_stopwords(row['English'], row['Hindi']), axis=1)\n",
    "\n",
    "df_subtitles[['English', 'Hindi', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88a7d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fcb66aeb50476bba6966bfb4cb3426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5c0b187fff4e88807756f77da6709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completing Translations:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "\n",
    "def select_and_translate_to_hindi(text, tokenizer, model):\n",
    "    \"\"\"Selects the last word or the last two words of a sentence and translates them to Hindi.\"\"\"\n",
    "    words = text.split()\n",
    "    # Randomly choose to take the last word or the last two words\n",
    "    num_words_to_translate = random.choice([1, 2])\n",
    "    words_to_translate = ' '.join(words[-num_words_to_translate:])\n",
    "    rest_of_sentence = ' '.join(words[:-num_words_to_translate])\n",
    "    \n",
    "    # Translate the selected part to Hindi\n",
    "    if words_to_translate.strip():  # Check if the text is not just empty or whitespace\n",
    "        inputs = tokenizer(words_to_translate, return_tensors=\"pt\", padding=True)\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=512, num_beams=4, early_stopping=True)\n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        translation = \"\"\n",
    "    \n",
    "    # Return the original (minus the last word(s)) and the translated part\n",
    "    return rest_of_sentence, translation\n",
    "\n",
    "def translate_batch_to_hindi(batch, tokenizer, model):\n",
    "    translations = []\n",
    "    for text in batch:\n",
    "        _, translated_text = select_and_translate_to_hindi(text, tokenizer, model)\n",
    "        translations.append(translated_text)\n",
    "    return translations\n",
    "\n",
    "# Load your tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "\n",
    "# Prepare for translation\n",
    "batch_size = 50  # Adjust based on your system\n",
    "translated_texts = []\n",
    "english_parts = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in tqdm(range(0, len(data['preprocessed_text']), batch_size), desc=\"Processing Batches\"):\n",
    "        batch = data['preprocessed_text'][i:i+batch_size].tolist()\n",
    "        futures.append(executor.submit(translate_batch_to_hindi, batch, tokenizer, model))\n",
    "\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Completing Translations\"):\n",
    "        translated_texts.extend(future.result())\n",
    "\n",
    "# Combine English and translated parts to form Hinglish sentences\n",
    "for idx, text in enumerate(data['preprocessed_text']):\n",
    "    num_words_to_keep = random.choice([1, 2])\n",
    "    words = text.split()\n",
    "    english_part = ' '.join(words[:-num_words_to_keep])\n",
    "    english_parts.append(english_part)\n",
    "\n",
    "data['Hinglish'] = [e + ' ' + h if h else e for e, h in zip(english_parts, translated_texts)]\n",
    "\n",
    "# Save to new CSV\n",
    "data.to_csv('hinglish_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, TimeDistributed, SpatialDropout1D, Attention, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Use Hinglish sentences as input and English sentences as output\n",
    "input_sentences = data['Hinglish']\n",
    "output_sentences = data['preprocessed_text']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(input_sentences) + list(output_sentences))\n",
    "input_sequences = tokenizer.texts_to_sequences(input_sentences)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_sentences)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in output_sequences))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding sequences\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "output_padded = pad_sequences(output_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_padded, output_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model building with added complexity\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=512, input_length=max_length),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Training the model with increased epochs\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "# To save the model for future use\n",
    "model.save('hinglish_to_english_translation_model_improved.h5')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a230fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, TimeDistributed, SpatialDropout1D, Attention, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Assuming `dataset` is your DataFrame with 'Hinglish' and 'English' columns loaded from 'final_dataset.csv'\n",
    "# Clean the text data\n",
    "data['Hinglish'] = data['Hinglish'].apply(clean_text)\n",
    "data['preprocessed_text'] = data['preprocessed_text'].apply(clean_text)\n",
    "\n",
    "# Use Hinglish sentences as input and English sentences as output\n",
    "input_sentences = data['Hinglish']\n",
    "output_sentences = data['preprocessed_text']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(input_sentences) + list(output_sentences))\n",
    "input_sequences = tokenizer.texts_to_sequences(input_sentences)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_sentences)\n",
    "\n",
    "# Find the maximum sequence length for padding\n",
    "max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in output_sequences))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Padding sequences\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "output_padded = pad_sequences(output_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Early Stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.001, mode='min', verbose=1)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_padded, output_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model building with added complexity\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=512, input_length=max_length),\n",
    "    SpatialDropout1D(0.2),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model with increased epochs\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    np.expand_dims(y_train, -1),  # Ensuring y_train is properly expanded for sparse_categorical_crossentropy\n",
    "    batch_size=64, \n",
    "    epochs=20, \n",
    "    validation_data=(X_test, np.expand_dims(y_test, -1)),  # Same for y_test\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# To save the model for future use\n",
    "model.save('hinglish_to_english_translation_model_improved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "plot_model(model, to_file='model_improved.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b264e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb714e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
