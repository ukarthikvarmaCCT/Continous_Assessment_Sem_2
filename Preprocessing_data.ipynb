{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080c333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/apple/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2777884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available: \", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313bc9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. CSV files have been combined based on video IDs and duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[संगीत\\]', '', text)\n",
    "    return re.sub(r'[^ऀ-ॿ ]', '', text).strip()\n",
    "\n",
    "def process_subtitle_file(srt_file_path):\n",
    "    subtitles = []\n",
    "    with open(srt_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "    for i in range(0, len(content), 4):\n",
    "        if i + 2 < len(content):\n",
    "            cleaned_line = clean_text(content[i + 2])\n",
    "            if cleaned_line and len(cleaned_line.split()) > 1:\n",
    "                subtitles.append([cleaned_line])\n",
    "    return subtitles\n",
    "\n",
    "def save_subtitles_to_csv(video_id, subtitles, subtitles_csv_dir):\n",
    "    if subtitles:\n",
    "        df_subtitles = pd.DataFrame(subtitles, columns=['Hindi'])\n",
    "        csv_file_path = os.path.join(subtitles_csv_dir, f\"{video_id}.csv\")\n",
    "        df_subtitles.to_csv(csv_file_path, index=False)\n",
    "\n",
    "def process_file(filename, subtitles_dir, subtitles_csv_dir):\n",
    "    if filename.endswith(\".srt\"):\n",
    "        video_id = filename[:-4]\n",
    "        srt_file_path = os.path.join(subtitles_dir, filename)\n",
    "        subtitles = process_subtitle_file(srt_file_path)\n",
    "        save_subtitles_to_csv(video_id, subtitles, subtitles_csv_dir)\n",
    "\n",
    "def read_video_ids(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def combine_csv_files(video_ids, output_file, subtitles_csv_dir):\n",
    "    csv_frames = []\n",
    "    for video_id in video_ids:\n",
    "        csv_path = os.path.join(subtitles_csv_dir, f\"{video_id}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            csv_frames.append(df)\n",
    "    combined_df = pd.concat(csv_frames)\n",
    "    combined_df.drop_duplicates(inplace=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "def parallel_process_files(files, subtitles_dir, subtitles_csv_dir):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_file, filename, subtitles_dir, subtitles_csv_dir) for filename in files]\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "# Example usage\n",
    "directories = {\n",
    "    \"video_ids_dir\": 'video_ids',\n",
    "    \"subtitles_dir\": 'subtitles',\n",
    "    \"subtitles_csv_dir\": 'Subtitles_csv',\n",
    "    \"output_dir\": 'Subtitles_csv_combined'\n",
    "}\n",
    "\n",
    "for dir_path in directories.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "files = os.listdir(directories['subtitles_dir'])\n",
    "parallel_process_files(files, directories['subtitles_dir'], directories['subtitles_csv_dir'])\n",
    "\n",
    "for filename in os.listdir(directories['video_ids_dir']):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        base_name = filename[:-4]\n",
    "        output_filename = f\"{base_name}_combined_data.csv\"\n",
    "        output_path = os.path.join(directories['output_dir'], output_filename)\n",
    "        video_ids_path = os.path.join(directories['video_ids_dir'], filename)\n",
    "        video_ids = read_video_ids(video_ids_path)\n",
    "        combine_csv_files(video_ids, output_path, directories['subtitles_csv_dir'])\n",
    "\n",
    "print(\"Process completed. CSV files have been combined based on video IDs and duplicates removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40da7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to count words in a sentence\n",
    "def count_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "# Base directory containing your CSV files\n",
    "directory_path = 'Subtitles_csv_combined'\n",
    "\n",
    "# Placeholder for combined data\n",
    "combined_data = pd.DataFrame(columns=['Hindi'])\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Filter sentences with 5 or more words\n",
    "        filtered_df = df[df['Hindi'].apply(count_words) >= 5]\n",
    "        \n",
    "        # Combine\n",
    "        combined_data = pd.concat([combined_data, filtered_df])\n",
    "\n",
    "# If the combined data has more than 2000 sentences, randomly select 2000\n",
    "if len(combined_data) > 2000:\n",
    "    combined_data = combined_data.sample(n=2000, random_state=1).reset_index(drop=True)\n",
    "else:\n",
    "    combined_data = combined_data.sample(frac=1, random_state=1).reset_index(drop=True)  # Shuffle if less than 2000\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "combined_file_path = 'combined_for_translation.csv'\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f'Combined file saved to {combined_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8f2378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/apple/anaconda3/lib/python3.11/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ce7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved back to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Initialize the translation pipeline\n",
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-hi-en')\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translation = translator(text, max_length=400)[0]['translation_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        translation = \"\"\n",
    "    return translation\n",
    "\n",
    "# Parallel translation function\n",
    "def parallel_translate(df, num_workers=5):\n",
    "    texts = df['Hindi'].tolist()\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_text = {executor.submit(translate_text, text): text for text in texts}\n",
    "        for future in as_completed(future_to_text):\n",
    "            text = future_to_text[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'{text} generated an exception: {exc}')\n",
    "            else:\n",
    "                df.loc[df['Hindi'] == text, 'English'] = result\n",
    "    return df\n",
    "\n",
    "# Path to your combined CSV file\n",
    "file_path = 'combined_for_translation.csv'\n",
    "\n",
    "# Read the combined CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'English' column exists\n",
    "df['English'] = ''\n",
    "\n",
    "# Translate in parallel and update the DataFrame\n",
    "df_updated = parallel_translate(df)\n",
    "\n",
    "# Save the updated DataFrame back to the same file\n",
    "df_updated.to_csv(file_path, index=False)\n",
    "print(f'Translation completed and saved back to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be366640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def create_hinglish(hindi, english):\n",
    "    english_words = english.split()\n",
    "    hindi_words = hindi.split()\n",
    "    # Decide the number of Hindi words to insert (up to 3 words or the length of the Hindi sentence)\n",
    "    num_inserts = min(3, len(hindi_words))\n",
    "    for _ in range(num_inserts):\n",
    "        if english_words:  # Ensure there are English words left to insert into\n",
    "            insert_pos = random.randint(0, len(english_words))\n",
    "            hindi_word_to_insert = random.choice(hindi_words)\n",
    "            english_words.insert(insert_pos, hindi_word_to_insert)\n",
    "            # Optional: remove the Hindi word from future consideration (if you want each Hindi word used only once)\n",
    "            hindi_words.remove(hindi_word_to_insert)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "# Apply the function to create a Hinglish column\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish(row['Hindi'], row['English']), axis=1)\n",
    "\n",
    "# Displaying the DataFrame with the new Hinglish sentences\n",
    "df_subtitles[['Hindi', 'English', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec9235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def create_hinglish_based_on_stopwords(english, hindi):\n",
    "    words = english.split()\n",
    "    stop_word_indices = [index for index, word in enumerate(words) if word.lower() in stop_words]\n",
    "\n",
    "    if not stop_word_indices:\n",
    "        # No stop words found, return the original sentence (you can decide how to handle this case)\n",
    "        return english\n",
    "\n",
    "    # Find pivot index for Hindi insertion\n",
    "    pivot_index = stop_word_indices[-1] + 1  # We split the sentence after the last stop word\n",
    "\n",
    "    english_part = \" \".join(words[:pivot_index])\n",
    "    hindi_part = \" \".join(hindi.split()[pivot_index:])  # Corresponding Hindi split\n",
    "\n",
    "    # Combine the English and Hindi parts\n",
    "    hinglish_sentence = english_part + ' ' + hindi_part\n",
    "    return hinglish_sentence\n",
    "\n",
    "# Apply the function to create the Hinglish sentences\n",
    "df_subtitles['Hinglish'] = df_subtitles.apply(lambda row: create_hinglish_based_on_stopwords(row['English'], row['Hindi']), axis=1)\n",
    "\n",
    "df_subtitles[['English', 'Hindi', 'Hinglish']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88a7d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 21:34:02.452248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3d8411cce243f0a47aa0d1a8658fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257cfc95165a4f59a580f03d25b65dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completing Translations:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ca36d7939342e180a7f428a05ef803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe8d8889c734599b15a0e765bb6d2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bc27a27a004a818cbc7ae05c614078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf811fc3209f483395700f1d2a79aee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f955483c1754cebaf668f5519e3a702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4612e788934b99b2344b7d27ccddda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bb315923044b76bd503c348b5698ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1607f69e733d4ba884665beaacec459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c8a2a9c129424caf6bed8677f725b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdc0585d5cd4d3394d78c87a8382ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f377d48eefe44f178a478087b06ece72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6682d44610243dcbc245bf6e16275f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89381ac790ea40ae8023fff13a558568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d50aa01a7a34f13b67658bcf3551c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58ae954e7f9400fb2294133e081c60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e29caa339745dfb2d95c0c8db86418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea46f90673c4e09a44b087966f1d9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346d71f341694aa4aa655dc2ab733a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c75a4e16a44dcc989181b0b7d56f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1945f915649445448d4591979948542f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d214635057e4ece9cbfcdccc0214864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad732da7f5a444f900e0b0ad4a01b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283fd92636784f4187ccf22f3d5854b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dd12c2c98a44c8b43f17d1e84835bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6ec6409e3d4e509ee110e12dbc50c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519aa52b57e941a5becd932b661ef1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34109cb926ab44fd8b6348cd570efc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c300be52f7b41e897062bb5157be157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28430feabda146caa5b1e4c5f44fbcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d94e82da49143979b3a66a14c5ec2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f54e7d4afb94739afd61f6759acf73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4906d1f615044ddb88938683474474ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90ceae517a14a41bb8287cbf4e79bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a7775a1c064b6db54269ff992ffb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8f598accee4250857d931441258a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d712dc41bb884d6885d8b280ebe37895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e41243a44c64ee1a6d28d0d78cfbf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cc9f388969431197a928de288f16bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb5d4960e8046259fd657fdc2cdc8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353ff0cc347a4a49b5b8b09bff13e556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define your stop words\n",
    "stop_words = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"d\",\n",
    "    \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\", \"hadn\",\n",
    "    \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\", \"shouldn\", \"wasn\",\n",
    "    \"weren\", \"won\", \"wouldn\"}  # Customize this list as needed\n",
    "\n",
    "def split_on_last_stopword(sentence, stop_words):\n",
    "    words = sentence.split()\n",
    "    last_stop_word_index = None\n",
    "    for i, word in enumerate(words):\n",
    "        if word.lower() in stop_words:\n",
    "            last_stop_word_index = i\n",
    "    if last_stop_word_index is not None and last_stop_word_index + 1 < len(words):\n",
    "        return \" \".join(words[:last_stop_word_index + 1]), \" \".join(words[last_stop_word_index + 1:])\n",
    "    else:\n",
    "        return sentence, \"\"\n",
    "\n",
    "def translate_to_hindi(texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "    translations = []\n",
    "    for text in tqdm(texts, desc=\"Translating\"):\n",
    "        if text.strip():  # Check if the text is not just empty or whitespace\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model.generate(inputs[\"input_ids\"], max_length=512, num_beams=4, early_stopping=True)\n",
    "            translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            translations.append(translation)\n",
    "        else:\n",
    "            translations.append(\"\")\n",
    "    return translations\n",
    "\n",
    "# Load the CSV file\n",
    "df_subtitles = pd.read_csv('combined_for_translation.csv')\n",
    "\n",
    "# Split English sentences and prepare them for translation\n",
    "df_subtitles['English_Part'], df_subtitles['To_Translate'] = zip(*df_subtitles['English'].apply(lambda x: split_on_last_stopword(x, stop_words)))\n",
    "\n",
    "batch_size = 50  # Adjust based on your system\n",
    "translated_texts = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for i in tqdm(range(0, len(df_subtitles['To_Translate']), batch_size), desc=\"Processing Batches\"):\n",
    "        batch = df_subtitles['To_Translate'][i:i+batch_size].tolist()\n",
    "        futures.append(executor.submit(translate_to_hindi, batch))\n",
    "\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Completing Translations\"):\n",
    "        translated_texts.extend(future.result())\n",
    "\n",
    "# Update DataFrame with translated texts\n",
    "df_subtitles['Translated'] = translated_texts\n",
    "\n",
    "# Combine to form Hinglish sentences\n",
    "df_subtitles['Hinglish'] = df_subtitles['English_Part'] + ' ' + df_subtitles['Translated']\n",
    "\n",
    "# Save to new CSV\n",
    "df_subtitles.to_csv('combined_for_translation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3fca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_for_translation.csv\") \n",
    "\n",
    "# Step 1: Filter out rows where English_Part and Hinglish are the same\n",
    "df_filtered = df[df['English_Part'] != df['Hinglish']]\n",
    "\n",
    "# Step 2: Drop unnecessary columns\n",
    "df_final = df_filtered.drop(columns=['English_Part', 'To_Translate', 'Translated'])\n",
    "\n",
    "# df_final now contains the cleaned dataset\n",
    "df_final.to_csv('final_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b18ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
