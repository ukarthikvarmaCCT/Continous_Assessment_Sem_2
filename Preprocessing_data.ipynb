{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "080c333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/apple/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers in /Users/apple/anaconda3/lib/python3.11/site-packages (4.39.1)\n",
      "Requirement already satisfied: filelock in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apple/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aadee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 13:21:30.769016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import concurrent.futures\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e728f9",
   "metadata": {},
   "source": [
    "## Preprocessing Text\n",
    "- Preprocessing the SRT file and combining them into one csv file based on thier video id and their filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "313bc9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed. CSV files have been combined based on video IDs and duplicates removed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SubtitlesProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the text cleaning UDF\n",
    "def clean_text_udf(text):\n",
    "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', text)  # Remove timestamps\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML-like tags if present\n",
    "    text = re.sub(r'[,.\\']', '', text)  # Remove commas, full stops, and apostrophes\n",
    "    text = re.sub(r'\\[Music\\]|\\[Applause\\]|\\[Laughter\\]', '', text)  # Remove common noise words\n",
    "    return re.sub(r'[^A-Za-z0-9 .,?!]', '', text).strip()  # Final clean-up\n",
    "\n",
    "clean_text = F.udf(clean_text_udf, StringType())\n",
    "\n",
    "hdfs_input_path = \"hdfs://namenode:9000/subtitles/\"\n",
    "hdfs_output_path = \"hdfs://namenode:9000/cleaned_subtitles/\"\n",
    "\n",
    "# Load the subtitle data\n",
    "df_subtitles = spark.read.text(hdfs_input_path)\n",
    "\n",
    "# Apply the cleaning function\n",
    "df_cleaned = df_subtitles.withColumn(\"cleaned_content\", clean_text(\"value\"))\n",
    "\n",
    "# Display the cleaned data\n",
    "df_cleaned.show()\n",
    "\n",
    "# Save the cleaned subtitles back to HDFS\n",
    "df_cleaned.select(\"cleaned_content\").write.text(hdfs_output_path)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a3631",
   "metadata": {},
   "source": [
    "- Randomly selecting 2000 english sentences from the csv file and combining into single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40da7650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, rand\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProcessSubtitles\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a User-Defined Function (UDF) for counting words in a sentence\n",
    "@udf(returnType=IntegerType())\n",
    "def count_words(sentence):\n",
    "    return len(sentence.split())\n",
    "\n",
    "# Base directory in HDFS containing  CSV files\n",
    "directory_path = 'hdfs://namenode:9000/cleaned_subtitles'\n",
    "\n",
    "# Read all CSV files from the directory into a DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(f\"{directory_path}/*.csv\")\n",
    "\n",
    "# Filter sentences with 5 or more words using the UDF\n",
    "filtered_df = df.filter(count_words(col(\"English\")) >= 5)\n",
    "\n",
    "# Randomly select 2000 sentences if more than 2000, or shuffle if fewer\n",
    "count = filtered_df.count()\n",
    "\n",
    "if count > 2000:\n",
    "    sampled_df = filtered_df.sample(False, 2000 / count)\n",
    "else:\n",
    "    sampled_df = filtered_df.orderBy(rand())\n",
    "\n",
    "#output path in HDFS for the processed data\n",
    "output_path = 'hdfs://namenode:9000/combined_for_translation.csv'\n",
    "\n",
    "# Saving the processed data back to HDFS\n",
    "sampled_df.coalesce(1).write.option(\"header\", \"true\").csv(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(f'Combined file saved to {output_path}')\n",
    "\n",
    "# Read the CSV file from HDFS\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"hdfs://namenode:9000/combined_for_translation.csv\")\n",
    "\n",
    "# Show the first few rows\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43147228",
   "metadata": {},
   "source": [
    "- Translating English Sentences into hindi sentences and saving them to the same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ce7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved back to combined_for_translation.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TranslationApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define UDF for Translation\n",
    "def translate_text_udf(text):\n",
    "    # Initialize the translation pipeline \n",
    "    translator = pipeline('translation', model='Helsinki-NLP/opus-mt-en-hi')\n",
    "    try:\n",
    "        translation = translator(text, max_length=400)[0]['translation_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        translation = \"\"\n",
    "    return translation\n",
    "\n",
    "translate_udf = udf(translate_text_udf, StringType())\n",
    "\n",
    "input_path = 'hdfs://namenode:9000/combined_for_translation.csv'\n",
    "output_path = 'hdfs://namenode:9000/combined_for_translation.csv'\n",
    "\n",
    "# Read the data\n",
    "df = spark.read.option(\"header\", \"true\").csv(input_path)\n",
    "\n",
    "# Translate text\n",
    "translated_df = df.withColumn(\"Hindi\", translate_udf(col(\"English\")))\n",
    "\n",
    "# Saving the translated data back to HDFS\n",
    "translated_df.write.option(\"header\", \"true\").csv(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(f'Translation completed and saved back to {output_path}')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4454d67",
   "metadata": {},
   "source": [
    "## Creating Hinglish Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88a7d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1412e1b5c14464ab9b0279694fbd43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3483b216ecfb40b79b5aef220b5c6ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completing Translations:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HinglishTranslationLargeData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "def select_and_translate(text):\n",
    "    \"\"\"Randomly selects 1-3 words from the end of `text`, translates them, and combines with the remaining English sentence.\"\"\"\n",
    "    # Split the text and select the last 1-3 words\n",
    "    words = text.split()\n",
    "    num_words_to_translate = random.choice([1, 2, 3])\n",
    "    words_to_translate = ' '.join(words[-num_words_to_translate:])\n",
    "    rest_of_sentence = ' '.join(words[:-num_words_to_translate])\n",
    "    \n",
    "    # Simulate translation (Replace this with actual translation logic)\n",
    "    inputs = tokenizer(words_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=40, num_beams=4, early_stopping=True)\n",
    "    translated_part = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Combine the untranslated part with the translated part\n",
    "    hinglish_sentence = rest_of_sentence + ' ' + translated_part if translated_part else text\n",
    "    return hinglish_sentence\n",
    "\n",
    "# Define UDF for Spark to apply the translation function\n",
    "translate_udf = F.udf(select_and_translate, StringType())\n",
    "\n",
    "# Define paths for reading from and writing to HDFS\n",
    "input_path = 'hdfs://namenode:9000/combined_for_translation.csv'\n",
    "output_path = 'hdfs://namenode:9000/hinglish_sentences'\n",
    "\n",
    "# Read the data into a Spark DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(input_path)\n",
    "\n",
    "# Apply the UDF to translate and create Hinglish sentences\n",
    "df_with_hinglish = df.withColumn(\"Hinglish\", translate_udf(F.col(\"English\")))\n",
    "\n",
    "# Save the DataFrame with Hinglish sentences back to HDFS\n",
    "df_with_hinglish.write.option(\"header\", \"true\").csv(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Translation completed and saved back to {output_path}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0eef38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
